{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f505251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA stride 2 conv with the default padding and ks will reduce the activation map dimension by half.\\nFormula: (n + 2*pad - ks)//stride + 1. \\nAs the activation map dimension reduces by half we double the number of filters. This results in no overall change in computation as the network gets deeper and deeper.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1\n",
    "\"\"\"\n",
    "A stride 2 conv with the default padding and ks will reduce the activation map dimension by half.\n",
    "Formula: (n + 2*pad - ks)//stride + 1. \n",
    "As the activation map dimension reduces by half we double the number of filters. This results in no overall change in computation as the network gets deeper and deeper.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcdc0d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe number of parameters grows quadratically with kernel size. \\nThis makes big convolution kernels not cost efficient enough. \\nLimiting the number of parameters, we are limiting the number of unrelated features possible\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2\n",
    "\"\"\"\n",
    "The number of parameters grows quadratically with kernel size. \n",
    "This makes big convolution kernels not cost efficient enough. \n",
    "Limiting the number of parameters, we are limiting the number of unrelated features possible\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2cbfe5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nActivation records are stored on the \"Activation Stack\" so that when the current function is executing, we have a record of \"where what came before\".\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q3\n",
    "\"\"\"\n",
    "Activation records are stored on the \"Activation Stack\" so that when the current function is executing, we have a record of \"where what came before\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c016ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAny tweak of this training loop is defined in a Callback to avoid over-complicating the code of the training loop, \\nand to make it easy to mix and match different techniques\\n>> Callback.__call__ (event_name)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q4\n",
    "\"\"\"\n",
    "Any tweak of this training loop is defined in a Callback to avoid over-complicating the code of the training loop, \n",
    "and to make it easy to mix and match different techniques\n",
    ">> Callback.__call__ (event_name)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d74b6116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOne of the major disadvantages of using activations above zero is the problem of vanishing gradient.\\nfor a very high or very low value, the derivative of the activation is very low.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q5\n",
    "\"\"\"\n",
    "One of the major disadvantages of using activations above zero is the problem of vanishing gradient.\n",
    "for a very high or very low value, the derivative of the activation is very low.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20706ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEach batch can be subject to meticulous quality control and assurances, potentially causing increased employee downtime. \\nIncreased storage costs for large quantities of produced products. Errors with the batch produced will incur wasted time and cost\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q6\n",
    "\"\"\"\n",
    "Each batch can be subject to meticulous quality control and assurances, potentially causing increased employee downtime. \n",
    "Increased storage costs for large quantities of produced products. Errors with the batch produced will incur wasted time and cost\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b652b888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIf your learning rate is set too low, training will progress very slowly as you are making very tiny updates to the weights in your network. \\nHowever, if your learning rate is set too high, it can cause undesirable divergent behavior in your loss function.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q7\n",
    "\"\"\"\n",
    "If your learning rate is set too low, training will progress very slowly as you are making very tiny updates to the weights in your network. \n",
    "However, if your learning rate is set too high, it can cause undesirable divergent behavior in your loss function.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "165b7bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\na large learning rate allows the model to learn faster, at the cost of arriving on a sub-optimal final set of weights. \\nA smaller learning rate may allow the model to learn a more optimal or even globally optimal set of weights but may take significantly longer to train\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q8\n",
    "\"\"\"\n",
    "a large learning rate allows the model to learn faster, at the cost of arriving on a sub-optimal final set of weights. \n",
    "A smaller learning rate may allow the model to learn a more optimal or even globally optimal set of weights but may take significantly longer to train\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dfd3a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nallows large weight changes in the beginning of the learning process and small changes or fine-tuning towards the end of the learning process\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q9\n",
    "\"\"\"\n",
    "allows large weight changes in the beginning of the learning process and small changes or fine-tuning towards the end of the learning process\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb16f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
